{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WljjH8K3s7kG"
      },
      "source": [
        "# gpt-prompt-engineer\n",
        "Автор: Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "Репозиторий: https://github.com/mshumer/gpt-prompt-engineer\n",
        "\n",
        "Инструмент подбирает оптимальный промпт для заданной задачи.\n",
        "\n",
        "Как пользоваться:\n",
        "1. Создайте файл _secrets.py из _secrets.example.py и укажите в нём OpenAI API ключ.\n",
        "2. Если нет доступа к GPT-4, в ячейке с конфигом замените `CANDIDATE_MODEL = 'gpt-4'` на `CANDIDATE_MODEL = 'gpt-3.5-turbo'`.\n",
        "3. В последней ячейке заполните описание задачи, до 15 тестовых случаев и количество генерируемых промптов.\n",
        "4. Запустите все ячейки. ИИ сгенерирует варианты промптов и по ELO выберет лучшие."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai==0.28 prettytable tqdm tenacity wandb -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dQmMZdkG_RA5"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "import time\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import wandb\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "try:\n",
        "    from _secrets import OPENAI_API_KEY\n",
        "except ImportError:\n",
        "    OPENAI_API_KEY = \"\"  # создайте _secrets.py из _secrets.example.py и укажите ключ\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "use_wandb = False # True — логировать конфиг и результаты в Weights & Biases\n",
        "\n",
        "use_portkey = False # True — логировать цепочки промптов и ответы в Portkey (https://portkey.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_gen_system_prompt = \"\"\"Твоя задача — генерировать системные промпты для GPT-4 по описанию сценария использования и тестовым случаям.\n",
        "\n",
        "Промпты должны быть для произвольных задач: заголовок лендинга, вводный абзац, решение задачи по математике и т.п.\n",
        "\n",
        "В сгенерированном промпте опиши на русском языке, как должна вести себя модель: что она получает на вход и что может выводить. Будь креативен, чтобы получить лучший результат. Напоминать модели, что она ИИ, не нужно.\n",
        "\n",
        "Оценка будет по качеству работы промпта — не жульничай: не включай в промпт детали из тестовых случаев. Промпты с примерами из тестов дисквалифицируются.\n",
        "\n",
        "Главное: в ответе должен быть только текст промпта, без пояснений и лишнего текста.\"\"\"\n",
        "\n",
        "\n",
        "ranking_system_prompt = \"\"\"Твоя задача — оценить качество двух ответов, сгенерированных разными промптами для одной и той же задачи.\n",
        "\n",
        "Тебе даны: описание задачи, тестовый запрос и два варианта ответа (A и B).\n",
        "\n",
        "Выбери лучший по качеству. Если лучше вариант A — ответь одной буквой 'A'. Если лучше вариант B — ответь одной буквой 'B'.\n",
        "\n",
        "«Лучше» значит заметно лучше, а не чуть-чуть. Будь строгим критиком: помечай как лучший только тот вариант, который действительно впечатляет больше.\n",
        "\n",
        "В ответе — только одна буква: A или B. Оценивай беспристрастно.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Коэффициент K для изменения рейтинга ELO\n",
        "K = 32\n",
        "\n",
        "CANDIDATE_MODEL = 'gpt-4o-mini'  # или gpt-4.1-mini, gpt-4o — см. доступ в OpenAI\n",
        "CANDIDATE_MODEL_TEMPERATURE = 0.9\n",
        "\n",
        "GENERATION_MODEL = 'gpt-4o-mini'\n",
        "GENERATION_MODEL_TEMPERATURE = 0.8\n",
        "GENERATION_MODEL_MAX_TOKENS = 60\n",
        "\n",
        "N_RETRIES = 3  # сколько раз повторять запрос к модели ранжирования при ошибке\n",
        "RANKING_MODEL = 'gpt-4o-mini'\n",
        "RANKING_MODEL_TEMPERATURE = 0.5\n",
        "\n",
        "NUMBER_OF_PROMPTS = 10 # сколько вариантов промптов генерировать (больше — дороже, но лучше результат)\n",
        "\n",
        "WANDB_PROJECT_NAME = \"gpt-prompt-eng\" # имя проекта в Weights & Biases при use_wandb = True\n",
        "WANDB_RUN_NAME = None # имя запуска в W&B для идентификации (опционально)\n",
        "\n",
        "PORTKEY_API = \"\" # API ключ Portkey при use_portkey = True: https://app.portkey.ai/\n",
        "PORTKEY_TRACE = \"prompt_engineer_test_run\" # ID трассы для отличия цепочек промптов\n",
        "HEADERS = {} # не менять — заполняется при use_portkey = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_wandb_run():\n",
        "  # запуск сессии wandb и сохранение конфига\n",
        "  wandb.init(\n",
        "    project=WANDB_PROJECT_NAME, \n",
        "    name=WANDB_RUN_NAME,\n",
        "    config={\n",
        "      \"K\": K,\n",
        "      \"system_gen_system_prompt\": system_gen_system_prompt, \n",
        "      \"ranking_system_prompt\": ranking_system_prompt,\n",
        "      \"candidate_model\": CANDIDATE_MODEL,\n",
        "      \"candidate_model_temperature\": CANDIDATE_MODEL_TEMPERATURE,\n",
        "      \"generation_model\": GENERATION_MODEL,\n",
        "      \"generation_model_temperature\": GENERATION_MODEL_TEMPERATURE,\n",
        "      \"generation_model_max_tokens\": GENERATION_MODEL_MAX_TOKENS,\n",
        "      \"n_retries\": N_RETRIES,\n",
        "      \"ranking_model\": RANKING_MODEL,\n",
        "      \"ranking_model_temperature\": RANKING_MODEL_TEMPERATURE,\n",
        "      \"number_of_prompts\": NUMBER_OF_PROMPTS\n",
        "      })\n",
        "  \n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# По желанию: логирование конфига, промптов и результатов в Weights & Biases\n",
        "if use_wandb:\n",
        "  start_wandb_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_portkey_run():\n",
        "  # заголовки Portkey для логирования промптов и ответов\n",
        "  openai.api_base=\"https://api.portkey.ai/v1/proxy\"\n",
        "  HEADERS = {\n",
        "    \"x-portkey-api-key\": PORTKEY_API, \n",
        "    \"x-portkey-mode\": \"proxy openai\",\n",
        "    \"x-portkey-trace-id\": PORTKEY_TRACE,\n",
        "    #\"x-portkey-retry-count\": 5 # авто-повторы с экспоненциальной задержкой при сбое OpenAI\n",
        "  } \n",
        "  return HEADERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# По желанию: логирование промптов и ответов в Portkey\n",
        "if use_portkey:\n",
        "    HEADERS=start_portkey_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wXeqMQpzzosx"
      },
      "outputs": [],
      "source": [
        "def generate_candidate_prompts(description, test_cases, number_of_prompts):\n",
        "  outputs = openai.ChatCompletion.create(\n",
        "      model=CANDIDATE_MODEL, # при отсутствии GPT-4 замените на 'gpt-3.5-turbo'\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": system_gen_system_prompt},\n",
        "          {\"role\": \"user\", \"content\": f\"Вот тестовые случаи: `{test_cases}`\\n\\nОписание сценария: `{description.strip()}`\\n\\nОтветь только текстом промпта, без пояснений. Будь креативен.\"}\n",
        "          ],\n",
        "      temperature=CANDIDATE_MODEL_TEMPERATURE,\n",
        "      n=number_of_prompts,\n",
        "      headers=HEADERS)\n",
        "\n",
        "  prompts = []\n",
        "\n",
        "  for i in outputs.choices:\n",
        "    prompts.append(i.message.content)\n",
        "  return prompts\n",
        "\n",
        "def expected_score(r1, r2):\n",
        "    return 1 / (1 + 10**((r2 - r1) / 400))\n",
        "\n",
        "def update_elo(r1, r2, score1):\n",
        "    e1 = expected_score(r1, r2)\n",
        "    e2 = expected_score(r2, r1)\n",
        "    return r1 + K * (score1 - e1), r2 + K * ((1 - score1) - e2)\n",
        "\n",
        "# Ранжирование: до N_RETRIES повторов с экспоненциальной задержкой.\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def get_score(description, test_case, pos1, pos2, ranking_model_name, ranking_model_temperature):    \n",
        "    score = openai.ChatCompletion.create(\n",
        "        model=ranking_model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": ranking_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Задача: {description.strip()}\n",
        "Запрос: {test_case['prompt']}\n",
        "Вариант A: {pos1}\n",
        "Вариант B: {pos2}\"\"\"}\n",
        "        ],\n",
        "        logit_bias={\n",
        "              '32': 100,  # токен 'A'\n",
        "              '33': 100,  # токен 'B'\n",
        "        },\n",
        "        max_tokens=1,\n",
        "        temperature=ranking_model_temperature,\n",
        "        headers=HEADERS,\n",
        "    ).choices[0].message.content\n",
        "    return score\n",
        "\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def get_generation(prompt, test_case):\n",
        "    generation = openai.ChatCompletion.create(\n",
        "        model=GENERATION_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"{test_case['prompt']}\"}\n",
        "        ],\n",
        "        max_tokens=GENERATION_MODEL_MAX_TOKENS,\n",
        "        temperature=GENERATION_MODEL_TEMPERATURE,\n",
        "        headers=HEADERS,\n",
        "    ).choices[0].message.content\n",
        "    return generation\n",
        "\n",
        "def test_candidate_prompts(test_cases, description, prompts):\n",
        "  # Начальный рейтинг ELO для каждого промпта — 1200\n",
        "  prompt_ratings = {prompt: 1200 for prompt in prompts}\n",
        "\n",
        "  # Число раундов для прогресс-бара\n",
        "  total_rounds = len(test_cases) * len(prompts) * (len(prompts) - 1) // 2\n",
        "\n",
        "  pbar = tqdm(total=total_rounds, ncols=70)\n",
        "\n",
        "  # Для каждой пары промптов\n",
        "  for prompt1, prompt2 in itertools.combinations(prompts, 2):\n",
        "      for test_case in test_cases:\n",
        "          pbar.update()\n",
        "\n",
        "          generation1 = get_generation(prompt1, test_case)\n",
        "          generation2 = get_generation(prompt2, test_case)\n",
        "\n",
        "          # Ранжируем ответы\n",
        "          score1 = get_score(description, test_case, generation1, generation2, RANKING_MODEL, RANKING_MODEL_TEMPERATURE)\n",
        "          score2 = get_score(description, test_case, generation2, generation1, RANKING_MODEL, RANKING_MODEL_TEMPERATURE)\n",
        "\n",
        "          score1 = 1 if score1 == 'A' else 0 if score1 == 'B' else 0.5\n",
        "          score2 = 1 if score2 == 'B' else 0 if score2 == 'A' else 0.5\n",
        "\n",
        "          score = (score1 + score2) / 2\n",
        "\n",
        "          r1, r2 = prompt_ratings[prompt1], prompt_ratings[prompt2]\n",
        "          r1, r2 = update_elo(r1, r2, score)\n",
        "          prompt_ratings[prompt1], prompt_ratings[prompt2] = r1, r2\n",
        "\n",
        "          if score > 0.5:\n",
        "              print(f\"Победитель: {prompt1}\")\n",
        "          elif score < 0.5:\n",
        "              print(f\"Победитель: {prompt2}\")\n",
        "          else:\n",
        "              print(\"Ничья\")\n",
        "\n",
        "  pbar.close()\n",
        "\n",
        "  return prompt_ratings\n",
        "\n",
        "\n",
        "def generate_optimal_prompt(description, test_cases, number_of_prompts=10, use_wandb=False): \n",
        "  if use_wandb:\n",
        "    wandb_table = wandb.Table(columns=[\"Промпт\", \"Рейтинг\"])\n",
        "    if wandb.run is None:\n",
        "      start_wandb_run()\n",
        "\n",
        "  prompts = generate_candidate_prompts(description, test_cases, number_of_prompts)\n",
        "  prompt_ratings = test_candidate_prompts(test_cases, description, prompts)\n",
        "\n",
        "  # Итоговая таблица рейтингов ELO\n",
        "  table = PrettyTable()\n",
        "  table.field_names = [\"Промпт\", \"Рейтинг\"]\n",
        "  for prompt, rating in sorted(prompt_ratings.items(), key=lambda item: item[1], reverse=True):\n",
        "      table.add_row([prompt, rating])\n",
        "      if use_wandb:\n",
        "         wandb_table.add_data(prompt, rating)\n",
        "\n",
        "  if use_wandb: # сохранить результаты в таблицу Weights & Biases и завершить сессию\n",
        "    wandb.log({\"prompt_ratings\": wandb_table})\n",
        "    wandb.finish()\n",
        "  print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJSSKFfV_X9F"
      },
      "source": [
        "# В ячейке ниже укажите описание задачи и тестовые случаи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "description = \"По запросу сгенерировать заголовок для лендинга.\" # такое описание обычно даёт хороший результат\n",
        "\n",
        "test_cases = [\n",
        "    {'prompt': 'Продвижение нового фитнес-приложения Smartly'},\n",
        "    {'prompt': 'Почему веганская диета полезна для здоровья'},\n",
        "    {'prompt': 'Запуск онлайн-курса по цифровому маркетингу'},\n",
        "    {'prompt': 'Запуск линейки экологичной одежды'},\n",
        "    {'prompt': 'Продвижение блога о бюджетных путешествиях'},\n",
        "    {'prompt': 'Реклама ПО для управления проектами'},\n",
        "    {'prompt': 'Презентация книги по изучению Python'},\n",
        "    {'prompt': 'Продвижение платформы для изучения языков'},\n",
        "    {'prompt': 'Реклама сервиса персональных планов питания'},\n",
        "    {'prompt': 'Запуск приложения для ментального здоровья и медитации'},\n",
        "]\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.config.update({\"description\": description, \n",
        "                        \"test_cases\": test_cases})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "PermissionError",
          "evalue": "Project `proj_xKWNFYgcA7UQIAIRSlXZ6ey3` does not have access to model `gpt-4.1-mini`",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_optimal_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUMBER_OF_PROMPTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mgenerate_optimal_prompt\u001b[39m\u001b[34m(description, test_cases, number_of_prompts, use_wandb)\u001b[39m\n\u001b[32m    107\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m wandb.run \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     start_wandb_run()\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m prompts = \u001b[43mgenerate_candidate_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m prompt_ratings = test_candidate_prompts(test_cases, description, prompts)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Итоговая таблица рейтингов ELO\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenerate_candidate_prompts\u001b[39m\u001b[34m(description, test_cases, number_of_prompts)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_candidate_prompts\u001b[39m(description, test_cases, number_of_prompts):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m   outputs = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCANDIDATE_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# при отсутствии GPT-4 замените на 'gpt-3.5-turbo'\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_gen_system_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mВот тестовые случаи: `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_cases\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m`\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mОписание сценария: `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdescription\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m`\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mОтветь только текстом промпта, без пояснений. Будь креативен.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m          \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCANDIDATE_MODEL_TEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m      \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumber_of_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m   prompts = []\n\u001b[32m     14\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m outputs.choices:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[39m, in \u001b[36mChatCompletion.create\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time.time() > start + timeout:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[39m, in \u001b[36mEngineAPIResource.create\u001b[39m\u001b[34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     **params,\n\u001b[32m    137\u001b[39m ):\n\u001b[32m    138\u001b[39m     (\n\u001b[32m    139\u001b[39m         deployment_id,\n\u001b[32m    140\u001b[39m         engine,\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m         api_key, api_base, api_type, api_version, organization, **params\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     response, _, api_key = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[32m    165\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\api_requestor.py:298\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    278\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m     method,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    287\u001b[39m ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    288\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.request_raw(\n\u001b[32m    289\u001b[39m         method.lower(),\n\u001b[32m    290\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m         request_timeout=request_timeout,\n\u001b[32m    297\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     resp, got_stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m.api_key\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\api_requestor.py:700\u001b[39m, in \u001b[36mAPIRequestor._interpret_response\u001b[39m\u001b[34m(self, result, stream)\u001b[39m\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    693\u001b[39m         \u001b[38;5;28mself\u001b[39m._interpret_response_line(\n\u001b[32m    694\u001b[39m             line, result.status_code, result.headers, stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    695\u001b[39m         )\n\u001b[32m    696\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result.iter_lines())\n\u001b[32m    697\u001b[39m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    706\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    707\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\api_requestor.py:765\u001b[39m, in \u001b[36mAPIRequestor._interpret_response_line\u001b[39m\u001b[34m(self, rbody, rcode, rheaders, stream)\u001b[39m\n\u001b[32m    763\u001b[39m stream_error = stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp.data\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m200\u001b[39m <= rcode < \u001b[32m300\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_error_response(\n\u001b[32m    766\u001b[39m         rbody, rcode, resp.data, rheaders, stream_error=stream_error\n\u001b[32m    767\u001b[39m     )\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[31mPermissionError\u001b[39m: Project `proj_xKWNFYgcA7UQIAIRSlXZ6ey3` does not have access to model `gpt-4.1-mini`"
          ]
        }
      ],
      "source": [
        "generate_optimal_prompt(description, test_cases, NUMBER_OF_PROMPTS, use_wandb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMYK+Pn5QaRzPmh3T5a9ca",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
